{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Network Notebook\n",
    "\n",
    "This notebook contains the definition for a basic network that will \n",
    "play tic tac toe. This notebook also contains code to train this \n",
    "network utilizing Q learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Tic Tac Toe code\n",
    "from game import TicTacToe\n",
    "\n",
    "# Make a game\n",
    "toe = TicTacToe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Network\n",
    "\n",
    "Below is the code to create a nerual network to play the \n",
    "tic tac toe game. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import Tensorflow\n",
    "import tensorflow as tf\n",
    "\n",
    "# Reset graph\n",
    "tf.reset_default_graph()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define settings for the network\n",
    "\n",
    "# Batch Size\n",
    "batch_size = None\n",
    "\n",
    "# Define settings\n",
    "board_size = toe.size * toe.size\n",
    "\n",
    "# Trying for simple DNN right now\n",
    "hidden = 9\n",
    "\n",
    "# Define output space size, one output for each space\n",
    "# Network will output the space it wants to play on\n",
    "action_space = board_size\n",
    "\n",
    "# Learning Rate\n",
    "learning_rate = 1e-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define network itself\n",
    "\n",
    "# Board Input\n",
    "board = tf.placeholder(dtype=tf.float32, \n",
    "                       shape=[batch_size, board_size],\n",
    "                       name=\"Board\")\n",
    "\n",
    "# Weights\n",
    "W = tf.Variable(tf.random_normal(shape=[hidden, action_space]),\n",
    "                name=\"Weights\")\n",
    "# Biases\n",
    "b = tf.Variable(tf.zeros(shape=[action_space]),\n",
    "                name=\"Biases\")\n",
    "\n",
    "# Output\n",
    "pred = tf.nn.bias_add(tf.matmul(board, W), b)\n",
    "\n",
    "# Action\n",
    "action = tf.argmax(pred,\n",
    "                   name=\"Action\",\n",
    "                   axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define training for the network\n",
    "\n",
    "# Targets for future learning\n",
    "targets = tf.placeholder(dtype=tf.float32,\n",
    "                         shape=[batch_size, action_space],\n",
    "                         name=\"Targets\")\n",
    "\n",
    "# Define loss as distance from targets squared\n",
    "loss = tf.reduce_mean(tf.square(pred - targets))\n",
    "\n",
    "# Define training function to minimize loss\n",
    "train_step = tf.train.AdamOptimizer(0.1).minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize network \n",
    "sess = tf.Session()\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Network\n",
    "\n",
    "This will define how the AI will learn from previous games. \n",
    "\n",
    "Basicailly twp steps - \n",
    "- Observation : play some games and record win or loss and assign rewards\n",
    "- Training    : based on observation and calculated rewards, update values in \n",
    "the neural network to better fit the problem\n",
    "\n",
    "Repeat oberservation for a given number of games. After making overservations, \n",
    "batch training to progress over previous steps. Repeat and train against \n",
    "previous version of self. Keep iterating until can consistently beat its \n",
    "previous version and  then make current version previous version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Play against random player for testing\n",
    "\n",
    "from game import random_player\n",
    "\n",
    "enemy = random_player\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define variables for training\n",
    "\n",
    "# Number of steps to train as a group\n",
    "train_batch_size = 1000\n",
    "\n",
    "# Games to play before training\n",
    "observation_size = 10000\n",
    "\n",
    "# Iterations of training per training set\n",
    "training_steps = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define method to save rewards and results from a game\n",
    "\n",
    "def label_step(reward, state, action):\n",
    "    return {\"state\":state,\"action\":action,\"reward\":reward}\n",
    "\n",
    "def get_reward(step):\n",
    "    return step[\"reward\"]\n",
    "\n",
    "def get_state(step):\n",
    "    return step[\"state\"]\n",
    "\n",
    "def get_action(step):\n",
    "    return step[\"action\"]\n",
    "\n",
    "def get_target(step):\n",
    "    return get_action(step) * get_reward(step)\n",
    "\n",
    "def label_states(reward, states, actions, decay = 0.85):\n",
    "    \"\"\"Processes sates from a game. States should be feed in order\n",
    "    of first trun, second turn, third tur, and so on... \n",
    "    \n",
    "    Only feed actions for one player.\n",
    "    \n",
    "    Returns a list of labeled steps\"\"\"\n",
    "    turns = len(states)\n",
    "    return [label_step(reward * decay ** (turns - i), states[i], actions[i]) for i in range(turns)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Q player based on previous functions\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Qplayer:\n",
    "    \n",
    "    def __init__(self, sess, action_op, train_op, board_input, target_input, action_space):\n",
    "        self.state_batch = []\n",
    "        self.action_batch = []\n",
    "        self.observations = []\n",
    "        \n",
    "        self.board_size = toe.size * toe.size\n",
    "        \n",
    "        self.sess = sess\n",
    "        self.train_op = train_op\n",
    "        self.action_op = action_op\n",
    "        self.board_input = board_input\n",
    "        self.target_input = target_input\n",
    "        self.action_space = action_space\n",
    "    \n",
    "    def get_player(self, record=False):\n",
    "        def get_move(board, player):\n",
    "            board_vector = board.get_board_vector(\n",
    "                                            lambda x:  1 if x == player \n",
    "                                            else  0 if x == toe.empty\n",
    "                                            else -1)\n",
    "                                      \n",
    "\n",
    "            act = self.sess.run(self.action_op, feed_dict={self.board_input:\n",
    "                                                           np.reshape(board_vector, [1,self.board_size])})\n",
    "            act = act[0]\n",
    "            \n",
    "            if record:\n",
    "                act_vector = np.eye(action_space)[act]\n",
    "                self.action_batch += [act_vector]\n",
    "                self.state_batch += [board_vector]\n",
    "            \n",
    "            return TicTacToe.make_move(player, act // board.size, act % board.size)\n",
    "        return get_move\n",
    "    \n",
    "    def process_game(self, reward):\n",
    "        self.observations.extend(label_states(reward, self.state_batch, self.action_batch))\n",
    "        self.state_batch.clear()\n",
    "        self.action_batch.clear()\n",
    "        \n",
    "    def train(self):\n",
    "        observations_vector = np.array(self.observations)\n",
    "        for step in range(training_steps):\n",
    "            mini_batch = observations_vector[np.random.choice(len(observations_vector),\n",
    "                                                              size=train_batch_size,\n",
    "                                                              replace=False)]\n",
    "            board_batch = []\n",
    "            target_batch = []\n",
    "            for step in mini_batch:\n",
    "                board_batch.append(get_state(step))\n",
    "                target_batch.append(get_target(step))\n",
    "            self.sess.run(self.train_op, \n",
    "                          feed_dict={\n",
    "                              self.board_input:np.array(board_batch),\n",
    "                              self.target_input:np.array(board_batch),\n",
    "                          })\n",
    "        self.observations.clear()\n",
    "    \n",
    "    def play_game(self, enemy, train=True):\n",
    "        winner = toe.play_game(self.get_player(train), enemy,\n",
    "                               \n",
    "                player_1_piece = 'X', player_2_piece = 'O',log=False)\n",
    "        if train:\n",
    "            reward = 0\n",
    "            if winner == 'X':\n",
    "                reward = 1\n",
    "            elif winner == 'O':\n",
    "                reward = -1\n",
    "            self.process_game(reward)\n",
    "        return (\"win\" if winner == 'X' \n",
    "                else \"tie\" if winner == toe.tie \n",
    "                else \"lose\")\n",
    "        \n",
    "    def win_ratio(self, enemy, games = 1000):\n",
    "        \"\"\"Get number of games won playing against an ememy\"\"\"\n",
    "        results = {\"win\":0, \"tie\":0, \"lose\":0}\n",
    "        for _ in range(games):\n",
    "            results[self.play_game(enemy, train=False)] += 1\n",
    "        return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Qplayer at 0x29ac9280ba8>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make Q player\n",
    "q_player = Qplayer(sess, action, train_step, board, targets, action_space)\n",
    "q_player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lose': 972, 'tie': 4, 'win': 24}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Play test 1000 games against random player\n",
    "q_player.win_ratio(enemy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Viewing win ration should not save observations\n",
    "q_player.observations\n",
    "# Result should be []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "played 1000 games\n",
      "played 2000 games\n"
     ]
    }
   ],
   "source": [
    "# Have Q player play against ranodm player for an observation size\n",
    "for i in range(observation_size):\n",
    "    q_player.play_game(enemy)\n",
    "    if (i + 1) % 1000 == 0:\n",
    "        print(\"played\", i + 1, \"games\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After playing for an observatin size, train the network\n",
    "q_player.train()\n",
    "# Play test 1000 games against random player\n",
    "q_player.win_ratio(enemy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat process 1000 times\n",
    "for _ in range(1000):\n",
    "    for i in range(observation_size):\n",
    "        q_player.play_game(enemy)\n",
    "        if (i + 1) % 1000 == 0:\n",
    "            print(\"played\", i + 1, \"games\")\n",
    "\n",
    "    # After playing for an observatin size, train the network\n",
    "    q_player.train()\n",
    "    # Play test 1000 games against random player\n",
    "    print(q_player.win_ratio(enemy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a game against the ai\n",
    "\n",
    "from game import human_player\n",
    "\n",
    "q_player.play_game(human_player, train=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
